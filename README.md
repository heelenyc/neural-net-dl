# Welcome to neural network!

### 全连接层架构的神经网络

#### 反向传播的公式说明

* 根据链式求导，定义$f_{L}$深度为L层的网络最后一层的梯度因子，它是从代价函数$C(a_{L})$传递到L层带权输出$z_L$的偏导，结合激活函数$a_{L} = \sigma({z_L})$ :

$$
f_{L} = \frac{\partial C}{\partial z_{L}} = (a_{L}-y)*\sigma^,(z_{L})
$$

* 因为 $z_L = w_L * x + b_L$，所以：

$$
\frac{\partial C}{\partial b_{L}} = f_L * 1  \qquad
\frac{\partial C}{\partial w_{L}} = f_L @ (a_{L-1})^T
$$

* 继续向L-1层传播

$$
f_{L-1} = \frac{\partial C}{\partial z_{L-1}} = (w_{L})^T @ f_{L} * \sigma^,(z_{L-1})
$$

$$
\frac{\partial C}{\partial b_{L-1}} = f_{L-1} * 1  \qquad
\frac{\partial C}{\partial w_{L-1}} = f_{L-1} @ (a_{L-2})^T
$$

#### 实验结果说明

* v1 版本，面向对象的版本
* v2 版本，集中在network里计算，没有按照预期那样出现更好的性能表现
* 关于激活函数
  1. 激活函数以及对应的导数值域在0到1之间分布，避免反向传播计算梯度时出现爆炸
  2. 在计算忽略最后一层的sigmoid激活函数在梯度中的偏导，意外获得了更快的学习，但理论上是有问题的，可能带来更大的震荡；
  3. 激活函数以及对应的导数要在0到1之间分布，其一用于归一，其二可以避免爆炸
  4. 激活函数连续可导，为了拟合计算（求导）可行
  5. sigmoid 激活函数最值是0.25，所以随着深度加大，反向传播中浅层的梯度迅速消失，所有有tanh、relu以及sinh等替代函数

* 关于层数和神经元数量
  1.更深的隐藏层或者更多的隐藏神经元不一定带来更好的效果
* 关于随机样本迭代
  1.小样本随机梯度可以加快学习，因为随机小样本迭代的特点：快速获得特征，小步调整，更敏捷；
  2.小样本的大小取值并不是越小越好，可能不能及时获取到特征
  3.小样本太大效果也不一定好，可能会拟合更多的非有效特征导致过拟合问题（跟样本拟合很好，代价很小，但是验证精度反而更低）
* 针对识别数字这个问题，全连接层架构的准确率极限在97%左右
* 效果跟初始化的参数有很大关系，开始效果比较差的情况下，学习进度没谱，可能要迭代很多次才能有明显进展；

#### 其他

* exp(z) 容易溢出，没有特别好的解决办法；更换激活函数；

